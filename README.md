# Taming Big Data with Apache Spark 4 and Python â€“ Hands On!

Este repositÃ³rio contÃ©m notebooks e arquivos de dados utilizados durante o curso **"Taming Big Data with Apache Spark 4 and Python â€“ Hands On!"**, oferecido pela **Sundog Education**.  
O conteÃºdo foi desenvolvido como parte do aprendizado prÃ¡tico de **Big Data Analysis** com **PySpark**, explorando desde conceitos bÃ¡sicos de RDDs atÃ© mÃ³dulos avanÃ§ados como **Spark SQL**, **Spark Streaming** e **GraphX**.

---

## ğŸ› ï¸ Tecnologias e Ferramentas

![Python](https://img.shields.io/badge/Python-3.8+-3776AB?style=for-the-badge&logo=python&logoColor=white)
![Apache Spark](https://img.shields.io/badge/Apache_Spark-4.x-E25A1C?style=for-the-badge&logo=apachespark&logoColor=white)
![PySpark](https://img.shields.io/badge/PySpark-Latest-E25A1C?style=for-the-badge&logo=apachespark&logoColor=white)
![Pandas](https://img.shields.io/badge/Pandas-150458?style=for-the-badge&logo=pandas&logoColor=white)
![Jupyter](https://img.shields.io/badge/Jupyter-F37626?style=for-the-badge&logo=jupyter&logoColor=white)
![Docker](https://img.shields.io/badge/Docker-2496ED?style=for-the-badge&logo=docker&logoColor=white)
![AWS](https://img.shields.io/badge/AWS_EMR-232F3E?style=for-the-badge&logo=amazonaws&logoColor=white)

**Principais tecnologias utilizadas:**
- **Apache Spark 4.x** - Framework de processamento distribuÃ­do
- **PySpark** - API Python para Spark
- **Pandas & Pandas-on-Spark** - AnÃ¡lise de dados e integraÃ§Ã£o hÃ­brida
- **Jupyter Notebook** - Ambiente interativo de desenvolvimento
- **Docker** - ContainerizaÃ§Ã£o do ambiente Spark
- **AWS EMR** - ExecuÃ§Ã£o em cluster na nuvem

---

## Sobre o Projeto

Durante o curso, foram estudadas tÃ©cnicas modernas para anÃ¡lise de grandes volumes de dados utilizando o **Apache Spark 4**, com Ãªnfase no uso de **PySpark** e **Pandas-on-Spark**.  

Os notebooks neste repositÃ³rio reproduzem os exercÃ­cios e exemplos prÃ¡ticos aprendidos, sendo material ideal para consulta, revisÃ£o e experimentaÃ§Ã£o.

**Principais objetivos de aprendizado:**

- Processar e transformar grandes conjuntos de dados com Spark
- Utilizar **DataFrames**, **Datasets** e **Spark SQL** para anÃ¡lise distribuÃ­da
- Implementar transformaÃ§Ãµes eficientes com **RDDs** (Resilient Distributed Datasets)
- Integrar Spark com **Pandas** para anÃ¡lise hÃ­brida de dados
- Executar jobs em clusters locais e na nuvem (AWS EMR)
- Explorar mÃ³dulos avanÃ§ados: **Spark ML**, **Streaming** e **GraphX**
- Aplicar boas prÃ¡ticas de otimizaÃ§Ã£o e paralelizaÃ§Ã£o de tarefas

---

## Estrutura do RepositÃ³rio

```
Spark_Notebooks/
â”‚
â”œâ”€â”€ Dados/                           # Arquivos de dados usados nos exercÃ­cios
â”‚   â”œâ”€â”€ ml-100k/                     # MovieLens Dataset (recomendaÃ§Ãµes de filmes)
â”‚   â”œâ”€â”€ 1800.csv
â”‚   â”œâ”€â”€ book.txt
â”‚   â”œâ”€â”€ customer-orders.csv
â”‚   â”œâ”€â”€ fakefriends.csv
â”‚   â””â”€â”€ fakefriends-header.csv
â”‚
â”œâ”€â”€ Spark_Basics/                    # ExercÃ­cios introdutÃ³rios sobre RDDs
â”‚   â”œâ”€â”€ 12.data100.ipynb
â”‚   â”œâ”€â”€ 14.friends.ipynb
â”‚   â”œâ”€â”€ 16.minTemperatures.ipynb
â”‚   â”œâ”€â”€ 17.maxTemperatures.ipynb
â”‚   â”œâ”€â”€ 18.bookTxt.ipynb
â”‚   â”œâ”€â”€ 20.bookSorted.ipynb
â”‚   â””â”€â”€ 22.byTotal.ipynb
â”‚
â”œâ”€â”€ Spark_DataFrame_DataSet/         # Exemplos com DataFrames e SQL
â”‚   â”œâ”€â”€ 25.sqlFunctions.ipynb
â”‚   â”œâ”€â”€ 26.dataFrames.ipynb
â”‚   â”œâ”€â”€ 28.friendsByAgeDataframe.ipynb
â”‚   â”œâ”€â”€ 29.wordCount.ipynb
â”‚   â”œâ”€â”€ 31.totalSpent.ipynb
â”‚   â”œâ”€â”€ 34.pandas.ipynb
â”‚   â”œâ”€â”€ 34.pandasConversion.ipynb
â”‚   â””â”€â”€ 34.pandas-transform-apply.ipynb
â”‚
â”œâ”€â”€ docker-compose.yaml              # ConfiguraÃ§Ã£o do ambiente Docker
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## Requisitos e InstalaÃ§Ã£o

### PrÃ©-requisitos

- **Python 3.8+**
- **Apache Spark 4.x**
- **Jupyter Notebook**
- **Docker** (opcional, para ambiente containerizado)

### InstalaÃ§Ã£o de DependÃªncias

```bash
# Criar ambiente virtual (recomendado)
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
# ou
.venv\Scripts\activate     # Windows

# Instalar pacotes necessÃ¡rios
pip install findspark pyspark pandas jupyter matplotlib
```

### Iniciando o Ambiente

```bash
# Iniciar Jupyter Notebook
jupyter notebook

# Ou via Docker
docker-compose up
```

Acesse o Jupyter Notebook via navegador em `http://localhost:8888`

---

## TÃ³picos Cobertos

O projeto reflete os principais mÃ³dulos estudados no curso:

### 1. Getting Started with Spark
InstalaÃ§Ã£o, configuraÃ§Ã£o e primeiros exemplos com PySpark. CriaÃ§Ã£o de sessÃµes Spark, carregamento de dados e operaÃ§Ãµes bÃ¡sicas.

### 2. Spark Basics and the Legacy RDD Interface
ManipulaÃ§Ã£o direta de RDDs (Resilient Distributed Datasets) â€” filtragem, mapeamento, agregaÃ§Ãµes e ordenaÃ§Ã£o distribuÃ­da.

### 3. SparkSQL, DataFrames, and DataSets
Uso de DataFrames, consultas SQL distribuÃ­das e criaÃ§Ã£o de Datasets tipados para anÃ¡lise estruturada de dados.

### 4. Advanced Examples of Spark Programs
Casos complexos de anÃ¡lise, incluindo sistemas de recomendaÃ§Ã£o (MovieLens) e anÃ¡lise de grafos sociais.

### 5. Running Spark on a Cluster
ExecuÃ§Ã£o de jobs em ambiente distribuÃ­do, incluindo configuraÃ§Ã£o e deploy em AWS EMR.

### 6. Machine Learning with Spark ML
IntroduÃ§Ã£o ao Spark MLlib para algoritmos de clustering, regressÃ£o e classificaÃ§Ã£o em escala.

### 7. Spark Streaming, Structured Streaming, and GraphX
Processamento de dados em tempo real e exploraÃ§Ã£o de relacionamentos complexos com GraphX.

### 8. Next Steps
ConclusÃ£o do curso e direcionamentos para aplicaÃ§Ã£o profissional do conhecimento adquirido.

---

## Conjuntos de Dados

Principais datasets utilizados nos notebooks:

| Dataset | DescriÃ§Ã£o |
|---------|-----------|
| **MovieLens 100k** | AnÃ¡lise de avaliaÃ§Ãµes e sistemas de recomendaÃ§Ã£o de filmes |
| **FakeFriends.csv** | Estudo de correlaÃ§Ã£o entre idade e nÃºmero de amigos |
| **Customer-Orders.csv** | CÃ¡lculo de valores totais gastos por cliente |
| **Book.txt** | Processamento de texto, word count e anÃ¡lise de frequÃªncia |
| **Temperature Data (1800.csv)** | IdentificaÃ§Ã£o de temperaturas mÃ­nimas e mÃ¡ximas por estaÃ§Ã£o |

---

## ExecuÃ§Ã£o via Docker

O arquivo `docker-compose.yaml` permite iniciar um ambiente Spark completo e containerizado:

```bash
docker-compose up -d
```

ApÃ³s a inicializaÃ§Ã£o, acesse:
- **Jupyter Notebook:** `http://localhost:8888`
- **Spark UI:** `http://localhost:4040` (quando um job estiver rodando)

---

## Objetivo do RepositÃ³rio

Este repositÃ³rio serve como:

- **Material de referÃªncia** para conceitos fundamentais e avanÃ§ados de Big Data com Spark
- **CÃ³digo prÃ¡tico** para experimentaÃ§Ã£o e aprendizado hands-on
- **Base de consulta** para revisÃ£o de tÃ©cnicas, padrÃµes e boas prÃ¡ticas
- **Ponto de partida** para desenvolvimento de projetos prÃ³prios com PySpark

---

## PrÃ³ximos Passos e Melhorias Futuras

PossÃ­veis expansÃµes planejadas para o projeto:

- [ ] Adicionar notebooks com exemplos prÃ¡ticos de **Spark Streaming** em tempo real
- [ ] Implementar pipelines completos de **Machine Learning** com Spark MLlib
- [ ] Criar casos de uso com dados reais obtidos de APIs pÃºblicas
- [ ] Adicionar exemplos de otimizaÃ§Ã£o de performance (particionamento, cache, broadcast variables)
- [ ] IntegraÃ§Ã£o com ferramentas de visualizaÃ§Ã£o (Plotly, Matplotlib, Seaborn)
- [ ] Exemplos de deploy e execuÃ§Ã£o em diferentes clouds (AWS EMR, Azure HDInsight, GCP Dataproc)
- [ ] DocumentaÃ§Ã£o de troubleshooting e resoluÃ§Ã£o de problemas comuns

---

## Como Contribuir

ContribuiÃ§Ãµes sÃ£o bem-vindas! Se vocÃª deseja adicionar exemplos, corrigir bugs ou melhorar a documentaÃ§Ã£o:

1. FaÃ§a um **fork** do projeto
2. Crie uma **branch** para sua feature (`git checkout -b feature/NovoExemplo`)
3. **Commit** suas mudanÃ§as (`git commit -m 'Adiciona novo exemplo de Streaming'`)
4. FaÃ§a o **push** para a branch (`git push origin feature/NovoExemplo`)
5. Abra um **Pull Request**

---

## LicenÃ§a e ObservaÃ§Ãµes

- Este projeto foi desenvolvido **para fins educacionais**, baseado nos exemplos prÃ¡ticos do curso **"Taming Big Data with Apache Spark 4 and Python â€“ Hands On!"**
- Os notebooks podem ser adaptados e utilizados em diferentes ambientes Spark (local, cluster ou cloud)
- Todo o cÃ³digo Ã© livre para estudo, experimentaÃ§Ã£o e adaptaÃ§Ã£o pessoal

---

## CrÃ©ditos

**Curso:** Taming Big Data with Apache Spark 4 and Python â€“ Hands On!  
**Oferecido por:** Sundog Education Team  
**Desenvolvido por:** Henrique MourÃ£o

---

## ğŸ“« Contato

- **GitHub:** [Henrique-Mourao](https://github.com/Henrique-Mourao)
- **LinkedIn:** [Henrique MourÃ£o](https://www.linkedin.com/in/henrique-mour%C3%A3o/)
- **Email:** henriquegmour4@gmail.com

---
